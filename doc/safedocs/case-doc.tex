\documentclass{article}

\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\etc}{\textit{etc.}}
\newcommand{\adhoc}{\textit{ad hoc}}

% Packages and abbreviations used by Konrad
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{amsthm}

\newcommand{\konst}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\imp}{\Rightarrow}
\newcommand{\set}[1]{\ensuremath{\{ {#1} \}}}
\newcommand{\kstar}[1]{\ensuremath{{#1}^{*}}}
\newcommand{\Lang}[1]{\ensuremath{{\mathcal L}({#1})}}
\newcommand{\Angled}[1]{\ensuremath{\langle {#1} \rangle}}

\theoremstyle{definition}
\newtheorem*{example}{Example}
%\newtheorem*{remark}{Remark}
\newtheorem{remark}{Remark}

\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes.multipart}

\begin{document}


\title{Ideas for SafeDocs-CASE Interaction}
\author{Konrad Slind \\ Trusted Systems Group \\ Collins Aerospace}
\date{\today}

\begin{abstract}
The SafeDocs and CASE projects have shared interests in expressive
message description languages, secure parsing, and verified
implementations. We discuss some research ideas in this area,
confining ourselves to a technical discussion.
\end{abstract}

\maketitle

\section*{Introduction}

The present author gave a talk to the SafeDocs PI meeting (November
10, 2020), titled \emph{Specifying Message Formats with Contiguity
  Types}. The talk provided an overview of the CASE project with a
focus on work formalizing and proving correct a message format
language and its parsing algorithm. The ensuing discussion revealed an
overlap in research concerns that should be explored.

\emph{Contiguity types} provide a small but expressive DSL for
specifying message formats, especially those with
\emph{self-describing} aspects such as length fields and
`unions'. Filters and parsers can be automatically generated from
contiguity type specifications, backed up by a formal correctness
proof. The syntax of contiguity types is given in Figure \ref{ctypes}.
In CASE, contiguity types have been used to implement the
``insert-filter" and ``insert-monitor" architectural transformations for
the set of UxAS message types. These messages can be very large and
complex; we were pleased to see that contiguity type specifications
capture UxAS message structure and constraints transparently.

We feel that the following aspects of contiguity types and message
parsing are worth pursuing in wider contexts:

\begin{description}

\item [First class `look-behind'] A common aspect of message parsing
  is that \emph{context}, \ie, already seen information in the
  message, is used to control further parsing steps. In our work, as a
  contiguity type parser processes a message, each field is added to
  the context (a finite map from C-style \emph{lvars} to message
  elements). This context is used in the computation of length fields,
  and in calculating which element of a union to
  choose. \footnote{Attribute grammars also offer a uniform naming
    scheme for accessing context. The relationship needs more study
    and we have been talking to the SRI SafeDocs performers about
    this.}

\item [In-message assertions] Message constraints can be expressed
  within a contiguity type. This allows data constraints to be
  expressed alongside the data in the message specification. In an
  implementation, this can be leveraged to provide fast failure and
  copy-free filtering.

\item [Context-sensitive sum] There is no unadorned `sum' operation
  representing the union of two formal languages in the syntax of
  contiguity types. Instead, there is a `guarded union` type
  \[
    \konst{Union}\; (\mathit{bexp}_1 : \tau_1) \ldots (\mathit{bexp}_n : \tau_n)
  \]
  where evaluation of the $\mathit{bexp}_i$, in the accumulated
  context, is used to determine which $\tau_i$ to continue parsing
  with. There are similar notions in other parser frameworks, \eg, PEG
  parsers and ANTLR, but those designs focus on \emph{lookahead}
  information, handling look-behind in an \adhoc{} way.

\end{description}


\begin{figure}
\[
\begin{array}{rcl}
 \mathit{base} & = & \konst{bool} \mid \konst{char} \mid \konst{u8} \mid
 \konst{u16} \mid \konst{u32} \mid \konst{u64}  \mid \konst{i16} \mid
 \konst{i32} \mid \konst{i64} \mid \konst{float} \mid \konst{double} \\
 \tau & = & \mathit{base} \\
      & \mid & \konst{Recd}\; (f_1 : \tau_1) \ldots (f_n : \tau_n) \\
      & \mid & \konst{Array}\; \tau \; \mathit{exp} \\
      & \mid & \konst{Union}\; (\mathit{bexp}_1 : \tau_1) \ldots (\mathit{bexp}_n : \tau_n)
\end{array}
\]
\caption{Contiguity types}
\label{ctypes}
\end{figure}

In the following, we describe a few ideas. Some are straightforward
but necessary building blocks, while others represent significant
advances.

\section{Generalized contiguity types}

In the current incarnation of contiguity types all data is bounded,
since each base type has a fixed size and all \konst{Array} types are
given an explicit bound. Removing these two restrictions would
greatly increase expressiveness. We have thus been exploring the
addition of a lexer and a version of Kleene star. The augmented syntax
can be seen in Figure \ref{gen-contig-types}.

\subsection{Lexing}

Currently, the set of base contiguity types comprises the usual base
types expected in most programming languages, see Figure
\ref{ctypes}. Semantically, a base type denotes a set of strings of
the specified width, but it is also coupled with an
\emph{interpretation function} for example, the contiguity type
\verb+u8+ denotes the set of all one-byte strings, interpreted by the
usual unsigned valuation function:

  \[ \konst{u8} = (\set{s \mid \konst{length}(s) = 1}, \konst{uvalFn} )
  \]

  This approach cannot, however, capture base types such as string
  literals of arbitrary size, or bignums, or the situation in packed
  bit-level encodings where fields are of \emph{ad hoc} sizes aimed at
  saving space. A common generalization is to express base types via
  regular expressions paired with interpretation functions. Now
  \konst{u8} can be defined as

  \[ \konst{u8} = ( . \; , \konst{uvalFn})
  \]

\noindent (where `.' is the standard regular expression denoting any character). Similarly,

  \[ \konst{Cstring} = ([\backslash 001-\backslash 255]^{*}\backslash 000, \lambda x.\,x )
  \]

\noindent denotes the set of zero-terminated strings, as found in the
C language. Its interpretation is just the identity function, but
could also be a function that drops the terminating \verb+\000+
character.

\begin{remark} [verification] There already exists a
  HOL4 theory of regexp based, maximal munch, lexer generation, and it
  should be relatively easy to adapt contiguity types to use those
  lexemes instead of the current restricted set of base types. One
  piece of important work yet to be done will be to make the lexer DFA
  be table-driven.
\end{remark}

\subsection{Kleene Star}
  Contiguity types lack Kleene star. The use of bounded \konst{Array}
  types provides much expressiveness for representing sequences of
  data, but ultimately some kinds of message can't be handled, \ie,
  those where there is no way to predict the number of nestings of
  structure: s-expressions, logical formulae, and programming language
  syntax trees are some typical examples. We address this shortcoming
  by providing a new contiguity type constructor---\konst{List}---
  of unbounded lists. A message matching a $\konst{List}\;\tau$ type
  will have an encoding similar to implementations of lists in
  functional languages. The recognition and parsing algorithms for
  contiguity types are extended to handle \konst{List} objects by
  iteratively unrolling the recursive equation

\[ \kstar{L} = \varepsilon \cup L \cdot \kstar{L} \]

Further work is needed to establish the details, but the following
sketch should give a sense of how we expect things to be.

\begin{figure}
\[
\begin{array}{rcl}
 \tau & =    & \konst{Base}\; (\mathit{regexp} \times \mathit{valFn}) \\
      & \mid & \konst{Recd}\; (f_1 : \tau_1) \ldots (f_n : \tau_n) \\
      & \mid & \konst{List}\; \tau \\
      & \mid & \konst{Array}\; \tau \; \mathit{exp} \\
      & \mid & \konst{Union}\; (\mathit{bexp}_1 : \tau_1) \ldots (\mathit{bexp}_n : \tau_n)
\end{array}
\]
\caption{Generalized contiguity types}
\label{gen-contig-types}
\end{figure}

\begin{example}[Recursive lists]

\end{example}
The type $\konst{List}\;\tau$ is represented by the following
contiguity type, a recursive record:

\[
 \konst{List}\;\tau =
   \left\{
     \begin{array}{lcl}
       \konst{tag} & : & \konst{u8} \\
       \konst{test} & : &
       \begin{array}[t] {lcl}
         \konst{Union} \{ \\
         \quad \konst{tag} = \konst{NilTag} & \longrightarrow & \varepsilon \\
         \quad \konst{tag} = \konst{ConsTag} & \longrightarrow &
          \{ \konst{hd} : \tau, \ \konst{tl} : \konst{List}\; \tau \} \, \}
        \end{array}
     \end{array}
   \right.
\]

In words, a $\konst{List}\;\tau$ matches a sequence of records where
a single-byte tag (\konst{NilTag} or \konst{ConsTag}) is read, then tested to see
whether to stop parsing the list (\konst{NilTag}) or to continue on to
parse a $\tau$ into the \konst{hd} field and recurse in order to
process the remainder of the list. Thus, the list of integers

\[ \konst{Cons}(1, \konst{Cons}(2, \konst{Cons}(3, \konst{Nil}))) \]

can be represented in a message as (\konst{Code} is an encoder for integers)

\[ \konst{ConsTag}\cdot \konst{Code}(1) \cdot
   \konst{ConsTag}\cdot \konst{Code}(2) \cdot
   \konst{ConsTag}\cdot \konst{Code}(3) \cdot \konst{NilTag} \]

and a contig-based parser, given type $\konst{List}\; \konst{int}$
(where $\konst{int}$ is a contiguity type for some flavor of integer)
would succeed, returning the context

\[
\begin{array}{rcl}
\konst{root.tag} & \mapsto & \konst{ConsTag} \\
\konst{root.hd} & \mapsto & \konst{Code}(1) \\
\konst{root.tl.tag} & \mapsto & \konst{ConsTag} \\
\konst{root.tl.hd} & \mapsto & \konst{Code}(2) \\
\konst{root.tl.tl.tag} & \mapsto & \konst{ConsTag} \\
\konst{root.tl.tl.hd} & \mapsto & \konst{Code}(3) \\
\konst{root.tl.tl.tl.tag} & \mapsto & \konst{NilTag}
\end{array}
\]

We expect this solution to be compositional, in the sense that
\konst{List} types can be the arguments of other contiguity types, can
be applied to themselves, \eg, $\konst{List}(\konst{List}\;\tau)$,
\etc \; Thus arbitrary branching structures of arbitrary finite depth
and width can be specified and parsed with this extension.  The
approach captures a certain class of \emph{context-free-like}
languages.\footnote{It isn't clear to us where in the Chomsky
  hierarchy this work falls.}  However, it differs distinctly from the
standard Chomsky hierarchy, mainly because sums are determined by
\emph{looking behind} when computing which choice to follow in a
\konst{Union} type; for example, the list parser sketched above
branches \emph{after} it has seen the tag. This approach does not
address any use of non-deterministic sum to express branching in
conventional grammars; we discuss the issue more in Section \ref{cfl}.

\begin{remark} [verification] There already exists a
  HOL4 theory of contiguity types, with a syntax, semantics, and
  parser generator correctness proof. Adding Kleene star to the syntax
  and semantics is straightforwad, and we have outlined above how the
  matcher would handle recursive lists.  The potentially unbounded
  iteration of Kleene star has often posed reasoning problems when
  formalizing matchers, but we have previous experience in
  circumventing such problems and are eager to extend the existing
  correctness proof.
\end{remark}



%% An important consideration is that---at least in the
%% embedded system context of CASE---messages are essentially flattened
%% versions of datastructures. Such formats are unambiguous and easy to
%% parse. This allows us to ignore some aspects of context-free parsing
%% such as infixity, associativity, and precedence. Implementations will
%% likely require a stack for parsing tree-shaped data, but the full
%% expressive power permitted by context-free grammars and parser
%% generators may not be needed, and may actually be too much.

\section{Conventional context-free parsing}
\label{cfl}

What of conventional context-free parsing technolgy? Is there a nice
marriage between the notions in contiguity types and parsing
techniques for context-free languages? We discuss two approaches in
the following: the first combination looks relatively simple while the
other seems to require a complete rebuild `from-the-ground-up'.

As an introduction to some of the issues, consider the following
grammar rules for JSON-style lists (confusingly called \konst{Array}
in JSON specifications). We assume the lexer has removed whitespace
and note that occurrences of open and close brackets in the rules are
terminals of the grammar.
\[
\begin{array}{rcl}
 \konst{Array}_{\konst{JSON}} & ::= & [\; ] \mid [ \konst{Elts} ] \\
 \konst{Elts}  & ::= & \konst{Value} \mid \konst{Value} ,  \konst{Elts} \\
\end{array}
\]

Without diving too deeply into details, a parser for this language has
to deal with a situation where it has read the initial open bracket
([) for an \konst{Array} and now needs to parse either a closing
  bracket (for the empty list) or a \konst{Value}. In this case, the
  parser needs lookahead to resolve the choice, and the look-behind of
  contiguity types isn't helpful. It could be that the difficulty can
  be avoided, by somehow pretending that the next lexeme is in the
  context, and thus available in the evaluation of boolean expressions
  of the \konst{Union} construct. But that raises other questions
  which are currently being worked on. It is likely that such
  questions find answers in the voluminous literature on parsing, \eg,
  the computation of ``first'' and ``follow'' sets in parser
  generators may be pertinent. It would, in any case, be a nice result
  if there was a smooth combination of look-behind and look-ahead
  parsing.

\subsection{Contiguity types as lexeme specifications}

Standard context-free grammars are typically presented by a set of
grammar rules phrased in terms of a collection of terminals and
non-terminals, where the terminals are specified by regular
expressions. An interesting idea is the following: replace regular
expressions by contiguity types in the specifications of terminals,
leaving the context-free constructions and algorithms untouched. Thus,
suppose we have a contiguity type specifying wellformed GPS
coordinates:

{\small
\begin{verbatim}
  gps = {lat : double
         lon : double
         alt : double
         check : Assert ...
        }
\end{verbatim}}

and wanted to specify a list of such. Then the following grammar rules
naturally express a binary encoded list of wellformed GPS
coordinates:
\[
\begin{array}{rcl}
 \konst{GPSList} & ::=  & \konst{NilTag} \\
                 & \mid & \konst{ConsTag} \cdot \konst{gps} \cdot \konst{GPSList} \\
\end{array}
\]

The lexeme \konst{gps} is produced by the contiguity type parser, as
are the lexemes for the tags, while the sequencing of terminals and
non-terminals, along with the recursion, is handled by the grammar
rules. Notice that lexemes are no longer simple atomic leaf nodes in
the parse tree: instead, a lexeme is now its own kind of parse tree,
one conforming to its specifying contiguity type. Thus a parse tree
derived from the context-free grammar is a two-level object: a parse
tree of parse trees, and so construction of parse trees from the
grammar must smoothly incorporate parse trees coming from the
lexer. This general approach would provide a nice example of combining
contiguity types and standard parser technology, while leaving both
implementations mostly unchanged.

\begin{remark} [verification] There already exists
  HOL4 theories of SLR and PEG parsing. Similarly, there are several
  verified parser generators in the Coq world. It would be interesting
  to marry that work with a verified contiguity type lexer to get a
  fully verified parser generator implementation with builtin
  assertion checking.
\end{remark}

\subsection{Thoroughly merging the ideas}

A much more challenging task would be to integrate the basic
contiguity type ideas `at the ground level' into standard
presentations of context-free languages. For example, one could add
the bounded \konst{Array} construct and the context-determined
\konst{Union} operators into the specification of the class of
context-free languages. Whether the result would be theoretically
nice, \eg, have decent closure properties, is an important
question. More generally, we would like to know of theories of formal
languages formulated under the assumption that \emph{the deeper into
  the input one goes, the more information one has for later
  decisions}. These sorts of questions are beyond our current
interests and abilities however.

\section{Stratified (modular) parsing}

 One common requirement in parsing is modularity: \eg, a parser for
 statements of a programming language is notionally parameterized by a
 parser for expressions. In most cases, these languages can be
 combined in the same grammar, but that works against worthy aims such
 as re-use and separation. The SafeDocs experience shows that this
 idea must be taken seriously for difficult languages such as
 PDF. What seems to be needed is a way to---in the midst of parsing
 language $A$---select a subsequence of the input to separately parse
 with the parser for language $B$. In determining the slice of the
 input to give to $B$, the accumulated context often needs to be
 consulted. Contiguity types give a way to handle this, and, in
 particular, may offer a framework in which the context accumulated by
 $B$ is added back to that of $A$. Network message processing, with
 its nesting of messages in layers of headers, has much the same
 flavor (albeit simpler).

\section{Efficient implementations}

Our current implementation of contiguity types is in an
\emph{interpreted} style: the evaluation of numeric \konst{Array}
bounds and boolean guards on \konst{Union} conditions is done with
respect to the current context, which is explicitly accumulated as the
message is processed. However, notice that the host programming
language (CakeML for us, but could be any programming language) will
no doubt already provide compilation for numeric and boolean
expressions. This leads to the idea of \emph{compiling contiguity
  types}: the current matching algorithm for contiguity types can be
replaced by the generation of equivalent host-language code which is
then compiled by the host-language compiler and evaluated. Although
the current contiguity type matcher has proved to be fast enough to
keep up with the real-time demands of the UxAS system, we expect that
a compiled version of the code would be much faster.

\begin{remark} [verification] We would like to formalize the compilation
algorithm and prove it correct. Since the CakeML formalization
provides an operational semantics for CakeML expressions, one should
be able to prove a correctness theorem. Suppose the following: a
contiguity type $\mathit{ctype}$ is represented as a CakeML datatype
element $\mathit{AST}_{\mathit{ctype}}$ ; function $\konst{Trans}$ is
a compilation pass that translates such elements to ordinary CakeML
ASTs; and \konst{App} is a $\mathit{AST}$ node that applies a function
to an argument. Then the following should be provable in the CakeML
semantics:

\[
 \konst{Eval}(\konst{Compile} (\konst{App}(\konst{Trans}\; \mathit{AST}_\mathit{ctype},s))) \iff s \in \Lang{\mathit{ctype}}
\]

In words, translating $\mathit{ctype}$ to a CakeML program, and
compiling and running its application to input string $s$, will
return a result that agrees with the semantics of contiguity types.

\end{remark}

Focusing only on filtering, \ie{} the \emph{Recognition} problem, we
have the concept of \emph{zero copy}, namely, that no data is copied
out of the input buffer in order to make the accept/reject verdict on
the input. Contiguity types are a natural fit for this since they
explicitly describe the message at the string (or byte array) level
using a notation that looks like high-level datastructures. We are
very interested in generating and measuring the performance of such
implementations.

\begin{remark} [verification]
There has recently been some work by the CakeML compiler group in
specifying and proving space bounds on CakeML programs. It would be
really interesting to be able to formally prove space bounds on a
copy-free filter implementation. However, their theory, anecdotally,
seems hard to apply to larger programs; it would be nice to work with
a motivated student to try to push the work ahead, using our examples.
\end{remark}


\section{Examples}

It is worth having some simple but challenging examples in mind.

\begin{enumerate}

\item A JSON parser that enforces the requirement that \textsf{Object}
  elements\footnote{An \textsf{Object} is essentially a finite map
    where the domain is strings.} have no duplicate keys. It is of
  course quite easy to enforce this property on the resulting parse
  tree, by making another pass over the data, but the challenge is to
  combine parsing and data validation in a single pass. How can we
  express this constraint inside the grammar, and generate an
  implementation that checks the constraint on the fly?

\item A filter for \emph{well-formed} first order terms where a
  function application is well-formed when the function is applied to
  the correct number of arguments. Thus input for the filter would
  have the form $S \cdot L$ where $S$ is a signature giving an arity
  to each function symbol, and $L$ is a list of terms. The constraint
  to be checked is that each term $t$ in $L$ needs to be (recursively)
  checked to ensure that each application $f(t_1,\ldots,t_n)$ is such
  that $f$ is in $S$, with arity $n$.  (Note there is another
  constraint, namely that $S$ has no duplicate function names, as in
  (1).)

\item Embedding the well-formed terms filter of (2) into the JSON parser of
  (1) in order to get a parser for well-formed terms encoded in
  JSON. This example captures a common pattern, for which JSON and XML
  for example are well-suited, namely using a general format to
  represent a specific language.

\end{enumerate}

\section{Conclusion}

The theory and practice of parsing is one of the great achievements of
Computer Science. Parsing technology continues to evolve and provide
new solutions to important problems, as we have seen in the SafeDocs
and CASE projects. Challenges remain, of course, and the ideas we have
discussed in this document have the potential to provide sound and
efficient solutions. This document has, obviously, a glaring lack of
examples of compelling interest to commercial and DoD stakeholders. It
would therefore be interesting to discuss relevant applications to
develop and apply our ideas on.



%% \section{Property-enhanced lexing}

%%    We have a formalized, proved correct lexer generator that uses the
%%    maximal-munch heuristic. To be more performant, it needs to be
%%    converted to be table-driven, and we would like it to be a
%%    property-producing lexer.

%%    Conventional lexers return a stream of lexemes. Regular expressions
%%    are able to enforce basic well-formedness properties of lexemes,
%%    and these would need to be propagated to the parser in order for
%%    the parser to enforce its own well-formedness constraints. This can
%%    be extended to stratified parsing situations (see below).


%% \paragraph{Contiguity types and lexing}

%%    Two ideas seem worth thinking about a little more. In one, we use
%%    contiguity types as a high-powered lexer language; in the other, we
%%    add a regexp-based lexer as a base contiguity type.

%% \begin{itemize}

%% \item Since contiguity types are deterministic, they could possibly be
%%       useful as a lexer replacement in certain parsing scenarios. A
%%       contig-type parser being used as a lexer would generate a "parse
%%       tree" lexeme that would be folded into the parser stack.

%% \item Contiguity types with regexp-based lexing added as a supplement
%%       to the existing suite of fixed-width base types. This amounts to
%%       a controlled use of Kleene star. This would add a nice way to
%%       handle C-style strings in messages, or any other such message
%%       portion of arbitrary length ending with a specific delimiter.

%% \end{itemize}

\end{document}
